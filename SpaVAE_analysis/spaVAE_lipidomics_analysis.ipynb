{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from collections import deque\n",
    "\n",
    "# Scientific computing and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Machine learning and clustering\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Single-cell and spatial analysis\n",
    "import scanpy as sc\n",
    "import harmonypy as hm\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "from umap import umap_ as umap\n",
    "\n",
    "# PyTorch and deep learning utilities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "\n",
    "# spaVAE-specific imports\n",
    "from spaVAE.spaVAE import SPAVAE\n",
    "from spaVAE.SVGP import SVGP\n",
    "from spaVAE.I_PID import PIDControl\n",
    "from spaVAE.VAE_utils import *\n",
    "from spaVAE.preprocess import normalize, geneSelection\n",
    "\n",
    "\n",
    "lipid_N_reduced_list = pd.read_csv('data/negative_globus_pallidus_ion_clustering_using_logfc_patterns_over_time.csv').iloc[:, 0].tolist()\n",
    "inducing_point_steps = 6\n",
    "loc_range = 20\n",
    "with open(\"data/lipid_names.txt\", \"r\") as file:\n",
    "    lipid_names = [line.strip() for line in file]\n",
    "def fetch_data(id):\n",
    "    return np.load('data/' + id + '_lipids.npy'), np.load('data/' + id + '_coords.npy')\n",
    "\n",
    "def extract_best_features(adata, reduced_list):\n",
    "    indices = np.array([elem in reduced_list for elem in adata.var_names.tolist()])\n",
    "    adata = adata[:, indices]\n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_preprocess_data(names):\n",
    "    dataset = {}\n",
    "    for sample in names:\n",
    "        lipidomics_matrix, coordinates_matrix = fetch_data(sample)\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        coordinates_matrix = scaler.fit_transform(coordinates_matrix) * loc_range\n",
    "\n",
    "        adata = sc.AnnData(lipidomics_matrix, dtype=\"float64\")\n",
    "        adata.var_names = lipid_names\n",
    "            \n",
    "        adata = extract_best_features(adata, lipid_N_reduced_list)\n",
    "        adata = normalize(adata, logtrans_input=False, normalize_input=False, total_ion_current=True, size_factors=False)\n",
    "        dataset[sample] = [adata, coordinates_matrix]\n",
    "    return dataset\n",
    "\n",
    "def normalize_across_brains(data_dict):\n",
    "    normalized_dict = {}\n",
    "    all_arrays = ()\n",
    "    original_shapes = []\n",
    "    for key, value in data_dict.items():\n",
    "        x = value[0]\n",
    "        original_shapes.append(x.shape)\n",
    "        all_arrays = all_arrays + (x.X,)\n",
    "    concatenated_array = np.concatenate(all_arrays, axis=0)     \n",
    "    dobject = sc.AnnData(concatenated_array)\n",
    "    dobject.raw = dobject.copy()\n",
    "        \n",
    "    sc.pp.log1p(dobject)\n",
    "    sc.pp.scale(dobject)\n",
    "    idx = 0\n",
    "    L = 0\n",
    "    for key, value in data_dict.items():\n",
    "        num_samples = original_shapes[L][0]\n",
    "        end_idx = idx + num_samples\n",
    "        dat_matrix = sc.AnnData(dobject.X[idx:end_idx], dtype=\"float64\")\n",
    "        dat_matrix.raw = sc.AnnData(dobject.raw.X[idx:end_idx], dtype=\"float64\") \n",
    "        normalized_dict[key] = [dat_matrix, value[1]]\n",
    "        idx = end_idx\n",
    "        L = L + 1\n",
    "    return normalized_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(datasets, maxiter, samp):\n",
    "    eps = 1e-5\n",
    "    initial_inducing_points = np.mgrid[0:(1+eps):(1./inducing_point_steps), 0:(1+eps):(1./inducing_point_steps)].reshape(2, -1).T * loc_range\n",
    "    print(initial_inducing_points.shape)\n",
    "\n",
    "        \n",
    "    noise = 0\n",
    "    dropoutE = 0\n",
    "    dropoutD = 0\n",
    "    encoder_layers = [256, 128, 64]\n",
    "    GP_dim = 1\n",
    "    Normal_dim = 9\n",
    "    decoder_layers = [64, 128, 256]\n",
    "    dynamicVAE = True\n",
    "    init_beta = 15\n",
    "    min_beta = 10\n",
    "    max_beta = 25\n",
    "    KL_loss = 1\n",
    "    fix_inducing_points = True\n",
    "    fixed_gp_params = True\n",
    "    kernel_scale = 20\n",
    "    device = 'cpu'\n",
    "\n",
    "    total_nt = 0\n",
    "    for dset in datasets:\n",
    "        total_nt = total_nt + dset[0].n_obs\n",
    "    vae = SPAVAE(input_dim=datasets[0][0].n_vars, \n",
    "                 GP_dim=GP_dim, Normal_dim=Normal_dim, \n",
    "                 encoder_layers=encoder_layers, decoder_layers=decoder_layers,\n",
    "            noise=noise, encoder_dropout=dropoutE, decoder_dropout=dropoutD,\n",
    "            fixed_inducing_points=fix_inducing_points, initial_inducing_points=initial_inducing_points, \n",
    "            fixed_gp_params=fixed_gp_params, kernel_scale=kernel_scale, N_train=total_nt, KL_loss=KL_loss, dynamicVAE=dynamicVAE, \n",
    "            init_beta=init_beta, min_beta=min_beta, max_beta=max_beta, dtype=torch.float64, device=device)\n",
    "    num_samples = 1\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-6\n",
    "    batch_size = 256\n",
    "    model_file = 'checkpoints/' + str(samp) + '.pt'\n",
    "    if not os.path.isfile(model_file): \n",
    "            t0 = time()\n",
    "            vae.train_model(pos=datasets[0][1], ncounts=datasets[0][0].X,\n",
    "                    lr=lr, weight_decay=weight_decay, batch_size=batch_size, num_samples=num_samples,\n",
    "                    maxiter=maxiter, save_model=True, model_weights=model_file, print_kernel_scale=False)\n",
    "            print('Training time: %d seconds.' % int(time() - t0))\n",
    "    else:\n",
    "            vae.load_model(model_file)\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(sample_id, pred, dis, shape=4):\n",
    "            refined_pred=[]\n",
    "            pred=pd.DataFrame({\"pred\": pred}, index=sample_id)\n",
    "            dis_df=pd.DataFrame(dis, index=sample_id, columns=sample_id)\n",
    "            num_nbs = shape\n",
    "            cccn = 0\n",
    "            for i in range(len(sample_id)):\n",
    "                index=sample_id[i]\n",
    "                dis_tmp=dis_df.loc[index, :].sort_values()\n",
    "                nbs=dis_tmp.iloc[0:(num_nbs+1)]\n",
    "                nbs_pred=pred.loc[nbs.index, \"pred\"]\n",
    "                self_pred=pred.loc[index, \"pred\"]\n",
    "                v_c=nbs_pred.value_counts()\n",
    "                if (v_c.loc[self_pred] <= num_nbs/2):\n",
    "                    refined_pred.append(v_c.idxmax())\n",
    "                    cccn = cccn + 1\n",
    "                else:           \n",
    "                    refined_pred.append(self_pred)\n",
    "                if (i+1) % 1000 == 0:\n",
    "                    print(\"Processed\", i+1, \"lines\")\n",
    "            return np.array(refined_pred)\n",
    "\n",
    "def leiden_clustering(data, res=0.5, n_neighbors=10):\n",
    "        adjacency_matrix = kneighbors_graph(data, n_neighbors=n_neighbors, mode='connectivity', include_self=False)\n",
    "        sources, targets = adjacency_matrix.nonzero()\n",
    "        weights = adjacency_matrix.data\n",
    "        g = ig.Graph(directed=False)\n",
    "        g.add_vertices(adjacency_matrix.shape[0])\n",
    "        g.add_edges(list(zip(sources, targets)))\n",
    "        g.es['weight'] = weights\n",
    "        partition = leidenalg.find_partition(g, leidenalg.RBConfigurationVertexPartition, resolution_parameter=res)\n",
    "        labels = partition.membership\n",
    "        return labels\n",
    "\n",
    "def get_colors_for_labels(labels):\n",
    "        unique_labels = np.unique(labels)\n",
    "        cmap = plt.get_cmap('tab20')\n",
    "        colors = [cmap(i) for i in range(len(unique_labels))]\n",
    "        label_to_color = {label: colors[i % len(colors)] for i, label in enumerate(unique_labels)}\n",
    "        color_list = [label_to_color[label] for label in labels]\n",
    "        return color_list\n",
    "\n",
    "def plot_embeddings_2d(latent, colors, neighbors):\n",
    "        umap_res = umap.UMAP(random_state=123, n_neighbors=neighbors).fit_transform(latent)\n",
    "        spaVAE_umap = pd.DataFrame(umap_res, columns=['X1', 'X2'])\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(x=spaVAE_umap['X1'], y=spaVAE_umap['X2'], color=colors,s=1, cmap='viridis')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(datasets, model, batch_ids, res=0.8, n_neigh=5):\n",
    "        final_latent = model.batching_latent_samples(X= datasets[0][1], Y= datasets[0][0].X)\n",
    "        pred_refined = np.array(leiden_clustering(final_latent, res=res, n_neighbors=n_neigh))\n",
    "        return final_latent, pred_refined\n",
    "\n",
    "def plot_results(datasets_comb, final_latent, clusters):\n",
    "    segmentation_colors = get_colors_for_labels(clusters)\n",
    "    idx = 0\n",
    "    for i in range(len(datasets_comb)):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(datasets_comb[i][1][:,0], datasets_comb[i][1][:, 1], c=segmentation_colors[idx: idx+ len(datasets_comb[i][1])], s=5)\n",
    "        plt.xlabel(\"X Coordinates\")\n",
    "        plt.ylabel(\"Y Coordinates\")\n",
    "        plt.show()\n",
    "        idx = idx + len(datasets_comb[i][1])\n",
    "\n",
    "    plot_embeddings_2d(final_latent, segmentation_colors, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = [0 for i in range(18)]\n",
    "brain_names = [['6moWT_0'], ['6moWT_1'], ['6moWT_2'], ['6moAD_0'], ['6moAD_1'], ['6moAD_2'], ['12moWT_0'], ['12moWT_1'], ['12moWT_2'], ['12moAD_0'], ['12moAD_1'], ['12moAD_2'], ['22moWT_0'], ['22moWT_1'], ['22moWT_2'], ['22moAD_0'], ['22moAD_1'], ['22moAD_2']]\n",
    "for i in range(len(brain_names)):\n",
    "    group_brain_names, group_batch_ids = brain_names[i], batch_ids[i]\n",
    "    data_arr = list(normalize_across_brains(fetch_preprocess_data(group_brain_names)).values())\n",
    "    model = train(data_arr, samp=group_brain_names, maxiter=50)\n",
    "    latent, clusters = get_clusters(data_arr, model, group_batch_ids)\n",
    "    plot_results(data_arr, latent, clusters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
